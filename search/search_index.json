{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#kokkos","title":"Kokkos","text":"<p>This repository hosts a collection of Kokkos exercises for the upcoming School on GPU Computing, organized by Cineca.</p> Name Description Level Setup: Installing Kokkos and Tutorial Environment This step guides you through cloning the Kokkos Core library and setting up the tutorial environment. Mandatory Exercise 0: Vector Addition Introduction to heterogeneous parallel programming beginner Exercise 1: Matrix-Vector Multiplication Introduction to Kokkos parallel patterns beginner Exercise 2: Matrix-Matrix Multiplication Introduction to Kokkos parallel patterns Intermediate Porting Heat Conduction Mini-App to Kokkos Put your Kokkos skills to the test by porting a mini-application simulating heat conduction. Give a description of the mini-app Advanced"},{"location":"#authors","title":"Authors","text":"<ul> <li>Michael Redenti - Initial work </li> </ul> <p>See also the list of contributors who participated in this project.</p>"},{"location":"#issues-feature-request","title":"Issues / Feature request","text":"<p>You can submit bug / issues / feature request using Tracker.</p>"},{"location":"#license","title":"License","text":"<p>TBD</p>"},{"location":"hello/","title":"Hello","text":""},{"location":"hello/#hello","title":"Hello","text":""},{"location":"tutorials/installation/","title":"Building Kokkos inline","text":"<p>For the tutorial, we will compile our Kokkos programs via a Makefile while building Kokkos inline. This allows us to easily swap between different default execution spaces and memory spaces.</p> Instructions: Cloning Kokkos Core repository <p>Change into your work area on Leonardo... <pre><code>cd $WORK</code></pre></p> <p>...define Kokkos release version/tag to clone... <pre><code>export KOKKOS_TAG=4.1.00</code></pre></p> <p>...clone the Kokkos repository... <pre><code>git clone --branch $KOKKOS_TAG https://github.com/kokkos/kokkos.git kokkos-$KOKKOS_TAG </code></pre></p> <p>...and finally export the path to the Kokkos folder. <pre><code>export KOKKOS_PATH=$PWD/kokkos-$KOKKOS_TAG</code></pre></p> Tip <p>To avoid having to export this environment variable every time you open a new shell, you might want to add it to your <code>~/.bashrc</code> file</p> Installing Kokkos as shared library/package <p>You may consult the documentation to learn about:   Building Kokkos as an intalled package Building Kokkos via Spack package manager but for the tutorial we will compile Kokkos programs inline via a Makefile</p> Instructions: Cloning the tutorial/exercises repository <p>No need as they are within the SYCL repository</p> <p>Change into your work area on Leonardo... <pre><code>cd $WORK</code></pre></p> <p>Next</p> <p>Great! We can now turn to running our first Kokkos program Tutorial 01: Vector Addition</p> Note <p>g </p> Abstract <p>g </p> Note <p>g </p> Success <p>g </p> Question <p>g </p> Failure <p>g </p> Danger <p>g </p> Bug <p>g </p> Example <p>g    </p> Quote <p>g </p> Click me ## Building Kokkos Tools **Clone Kokkos Tools repository** Define Kokkos Tools version/tag to clone  <pre><code>export KOKKOS_TOOLS_TAG=2.5.00</code></pre> Download repository <pre><code>git clone --branch $KOKKOS_TOOLS_TAG https://github.com/kokkos/kokkos-tools.git kokkos-tools-$KOKKOS_TOOLS_TAG </code></pre> and export the path to the Kokkos folder (you might want to source the full path such that you can activate the full path for every session) <pre><code>export KOKKOS_TOOLS_PATH=$PWD/kokkos-$KOKKOS_TOOLS_TAG\nmake CUDA_ROOT=$NVHPC_HOME/Linux_x86_64/22.3/cuda/</code></pre>  **Vtune connector** <pre><code>make VTUNE_HOME=$INTEL_ONEAPI_VTUNE_HOME/vtune/2021.7.1</code></pre>  You must enable `Kokkos` wih `Kokkos_ENABLE_LIBDL=ON` to load profiling hooks dynamically. To use one of the tools shipped with this repository you have to compile it, which will generate a dynamic library.  Before executing the Kokkos application you then have to set the environment variable `KOKKOS_TOOLS_LIBS` to point to the dynamic library e.g. in the bash shell:  <pre><code>export KOKKOS_TOOLS_LIBS=${HOME}/kokkos-tools/src/tools/memory-events/kp_memory_event.so</code></pre>  Explicit instrumentation:  <pre><code>Kokkos::Profiling::pushRegion(\"foo\");\nfoo();\nKokkos::Profiling::popRegion();</code></pre>"},{"location":"tutorials/installation/#building-kokkos-kernels","title":"Building Kokkos Kernels","text":""},{"location":"tutorials/installation/#useful-spack-commands","title":"Useful spack commands","text":"<p>Below is a short list of some useful commands. For the complete list of Spack commands check this link.      </p> Command Comment <code>spack env create -d .</code> To create a new environment in the current folder <code>spack env activate -p .</code> To activate an environment in the current folder Warning <p>STILL WORK IN PROGRESS: no configuration files and job submission script yet!</p> Tip <p>To avoid running the same command every time you open a new terminal, you might want to add this line to your ~/.bashrc file: <pre><code>. /&lt;FULLPATH-TO-SPACK-ROOT&gt;/spack/share/spack/setup-env.sh</code></pre></p>"},{"location":"tutorials/vectorAdd/","title":"Exercise 0: Vector Addition","text":"<p>In this first tutorial we will be profile the performance of vector addition written in Kokkos.  </p> <pre><code>module load cuda/11.8 gcc/11.3.0\nmake\nsrun -N 1 --ntasks-per-node=1 --cpus-per-task=1 --gres=gpu:1 -A cin_staff -p boost_usr_prod --time=00:30:00 -\n-pty /bin/bash\nOMP_PROC_BIND=spread OMP_PLACES=threads ./vector_add.x</code></pre>"},{"location":"tutorials/vectorAdd/#tutorial-01-vector-addition","title":"Tutorial 01: Vector Addition","text":""},{"location":"tutorials/vectorAdd/#introduction","title":"Introduction","text":"<p>This tutorial is an introduction for writing your first C++-Kokkos program and offload computation to a GPU. </p>"},{"location":"tutorials/vectorAdd/#a-quick-comparison-between-cuda-and-c","title":"A quick comparison between CUDA and C","text":"<p>Following table compares a hello world program in C and CUDA side-by-side. </p> <p>C</p> <pre><code>void c_hello(){\n    printf(\"Hello World!\\n\");\n}\n\nint main() {\n    c_hello();\n    return 0;\n}</code></pre> <p>CUDA <pre><code>__global__ void cuda_hello(){\n    printf(\"Hello World from GPU!\\n\");\n}\n\nint main() {\n    cuda_hello&lt;&lt;&lt;1,1&gt;&gt;&gt;(); \n    return 0;\n}</code></pre></p> CC++ <pre><code>#include &lt;stdio.h&gt;\n\nint main(void) {\n  printf(\"Hello world!\\n\");\n  return 0;\n}</code></pre> <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}</code></pre> <p>Example</p> Unordered ListOrdered List <pre><code>* Sed sagittis eleifend rutrum\n* Donec vitae suscipit est\n* Nulla tempor lobortis orci</code></pre> <pre><code>1. Sed sagittis eleifend rutrum\n2. Donec vitae suscipit est\n3. Nulla tempor lobortis orci</code></pre> <p>KOKKOS</p> <pre><code>#include &lt;Kokkos_core.hpp&gt;\n\nint main(int argc, char* argv) {\n\n    Kokkos::initialize(argc, argv);\n\n\n    Kokkos::finalize();\n\n    return 0;\n}</code></pre> <p>The major difference between C and CUDA implementation is <code>__global__</code> specifier and <code>&lt;&lt;&lt;...&gt;&gt;&gt;</code> syntax. The <code>__global__</code> specifier indicates a function that runs on device (GPU). Such function can be called through host code, e.g. the <code>main()</code> function in the example, and is also known as \"kernels\". </p> <p>When a kernel is called, its execution configuration is provided through <code>&lt;&lt;&lt;...&gt;&gt;&gt;</code> syntax, e.g. <code>cuda_hello&lt;&lt;&lt;1,1&gt;&gt;&gt;()</code>. In CUDA terminology, this is called \"kernel launch\". We will discuss about the parameter <code>(1,1)</code> later in this </p>"},{"location":"tutorials/vectorAdd/#compiling-cuda-programs","title":"Compiling CUDA programs","text":"<p>Compiling a CUDA program is similar to C program. NVIDIA provides a CUDA compiler called <code>nvcc</code> in the CUDA toolkit to compile CUDA code, typically stored in a file with extension <code>.cu</code>. For example</p> <pre><code>$&gt; nvcc hello.cu -o hello</code></pre> <p>You might see following warning when compiling a CUDA program using above command</p> <pre><code>nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).</code></pre> <p>This warning can be ignored as of now. </p>"},{"location":"tutorials/vectorAdd/#putting-things-in-actions","title":"Putting things in actions.","text":"<p>The CUDA hello world example does nothing, and even if the program is compiled, nothing will show up on screen. To get things into action, we will looks at vector addition. </p> <p>Following is an example of vector addition implemented in C . The example computes the addtion of two vectors stored in array <code>a</code> and <code>b</code> and put the result in array <code>out</code>.</p> <pre><code>#define N 10000000\n\nvoid vector_add(float *out, float *a, float *b, int n) {\n    for(int i = 0; i &lt; n; i++){\n        out[i] = a[i] + b[i];\n    }\n}\n\nint main(){\n    float *a, *b, *out; \n\n    // Allocate memory\n    a   = (float*)malloc(sizeof(float) * N);\n    b   = (float*)malloc(sizeof(float) * N);\n    out = (float*)malloc(sizeof(float) * N);\n\n    // Initialize array\n    for(int i = 0; i &lt; N; i++){\n        a[i] = 1.0f; b[i] = 2.0f;\n    }\n\n    // Main function\n    vector_add(out, a, b, N);\n}\n</code></pre>"},{"location":"tutorials/vectorAdd/#exercise-converting-vector-addition-to-cuda","title":"Exercise: Converting vector addition to CUDA","text":"<p>In the first exercise, we will convert <code>vector_add.c</code> to CUDA program <code>vector_add.cu</code> by using the hello world as example.</p> <ol> <li>Copy <code>vector_add.c</code> to <code>vector_add.cu</code></li> </ol> <pre><code>$&gt; cp vector_add.c vector_add.cu</code></pre> <ol> <li>Convert <code>vector_add()</code> to GPU kernel</li> </ol> <pre><code>__global__ void vector_add(float *out, float *a, float *b, int n) {\n    for(int i = 0; i &lt; n; i++){\n        out[i] = a[i] + b[i];\n    }\n}</code></pre> <ol> <li>Change <code>vector_add()</code> call in <code>main()</code> to kernel call</li> </ol> <pre><code>vector_add&lt;&lt;&lt;1,1&gt;&gt;&gt;(out, a, b, N);</code></pre> <ol> <li>Compile and run the program <pre><code>$&gt; nvcc vector_add.c -o vector_add\n$&gt; ./vector_add</code></pre></li> </ol> <p>You will notice that the program does not work correctly. The reason is CPU and GPUs are separate entities. Both have their own memory space. CPU cannot directly access GPU memory, and vice versa. In CUDA terminology, CPU memory is called host memory and GPU memory is called device memory. Pointers to CPU and GPU memory are called host pointer and device pointer, respectively. </p> <p>For data to be accessible by GPU, it must be presented in the device memory. CUDA provides APIs for allocating device memory and data transfer between host and device memory. Following is the common workflow of CUDA programs. </p> <ol> <li>Allocate host memory and initialized host data</li> <li>Allocate device memory</li> <li>Transfer input data from host to device memory</li> <li>Execute kernels</li> <li>Transfer output from device memory to host</li> </ol> <p>So far, we have done step 1 and 4. We will add step 2, 3, and 5 to our vector addition program and finish this exercise. </p>"},{"location":"tutorials/vectorAdd/#device-memory-management","title":"Device memory management","text":"<p>CUDA provides several functions for allocating device memory. The most common ones are <code>cudaMalloc()</code> and <code>cudaFree()</code>. The syntax for both functions are as follow</p> <pre><code>cudaMalloc(void **devPtr, size_t count);\ncudaFree(void *devPtr);</code></pre> <p><code>cudaMalloc()</code> allocates memory of size <code>count</code> in the device memory and updates the device pointer <code>devPtr</code> to the allocated memory. <code>cudaFree()</code> deallocates a region of the device memory where the device pointer <code>devPtr</code> points to. They are comparable to <code>malloc()</code> and <code>free()</code> in C, respectively</p>"},{"location":"tutorials/vectorAdd/#memory-transfer","title":"Memory transfer","text":"<p>Transfering data between host and device memory can be done through <code>cudaMemcpy</code> function, which is similar to <code>memcpy</code> in C. The syntax of <code>cudaMemcpy</code> is as follow</p> <pre><code>cudaMemcpy(void *dst, void *src, size_t count, cudaMemcpyKind kind)</code></pre> <p>The function copy a memory of size <code>count</code> from <code>src</code> to <code>dst</code>. <code>kind</code> indicates the direction. For typical usage, the value of <code>kind</code> is either <code>cudaMemcpyHostToDevice</code> or <code>cudaMemcpyDeviceToHost</code>. There are other possible values but we will not touch them in this tutorial. </p>"},{"location":"tutorials/vectorAdd/#exercise-cont-completing-vector-addition","title":"Exercise (Con't): Completing vector addition","text":"<ol> <li> <p>Allocate and deallocate device memory for array <code>a</code>, <code>b</code>, and <code>out</code>. </p> </li> <li> <p>Transfer <code>a</code>, <code>b</code>, and <code>out</code> between host and device memory. </p> <ul> <li>Quiz: Which array must be transferred before and after kernel execution ?</li> </ul> </li> </ol>"},{"location":"tutorials/vectorAdd/#example-solution-for-array-a","title":"Example: Solution for array 'a'","text":"<pre><code>void main(){\n    float *a, *b, *out;\n    float *d_a;\n\n    a = (float*)malloc(sizeof(float) * N);\n\n    // Allocate device memory for a\n    cudaMalloc((void**)&amp;d_a, sizeof(float) * N);\n\n    // Transfer data from host to device memory\n    cudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice);\n\n    \u2026\n    vector_add&lt;&lt;&lt;1,1&gt;&gt;&gt;(out, d_a, b, N);\n    \u2026\n\n    // Cleanup after kernel execution\n    cudaFree(d_a);\n    free(a);\n}</code></pre> <ol> <li>Compile and measure performance. </li> </ol> <pre><code>$&gt; nvcc vector_add.cu -o vector_add\n$&gt; time ./vector_add</code></pre>"},{"location":"tutorials/vectorAdd/#profiling-performance","title":"Profiling performance","text":"<p>Using <code>time</code> does not give much information about the program performance. NVIDIA provides a commandline profiler tool called <code>nvprof</code>, which give a more insight information of CUDA program performance.  </p> <p>To profile our vector addition, use following command</p> <pre><code>$&gt; nvprof ./vector_add</code></pre> <p>Following is an example profiling result on Tesla M2050</p> <pre><code>==6326== Profiling application: ./vector_add\n==6326== Profiling result:\nTime(%)      Time     Calls       Avg       Min       Max  Name\n 97.55%  1.42529s         1  1.42529s  1.42529s  1.42529s  vector_add(float*, float*, float*, int)\n  1.39%  20.318ms         2  10.159ms  10.126ms  10.192ms  [CUDA memcpy HtoD]\n  1.06%  15.549ms         1  15.549ms  15.549ms  15.549ms  [CUDA memcpy DtoH]</code></pre>"},{"location":"tutorials/vectorAdd/#wrap-up","title":"Wrap up","text":"<p>In this tutorial, we demonstrate how to write a simple vector addition in CUDA. We introduced GPU kernels and its execution from host code. Moreover, we introduced the concept of separated memory space between CPU and GPU. We also demonstrate how to manage the device memory. </p> <p>However, we still not run program in parallel. The kernel execution configuration <code>&lt;&lt;&lt;1,1&gt;&gt;&gt;</code> indicates that the kernel is launched with only 1 thread. In the next tutorial ../tutorial02/, we will modify vector addition to run in parallel. </p>"},{"location":"tutorials/vectorAdd/#acknowledgments","title":"Acknowledgments","text":"<ul> <li>Contents are adopted from An Even Easier Introduction to CUDA by Mark Harris, NVIDIA and CUDA C/C++ Basics by Cyril Zeller, NVIDIA. </li> </ul>"}]}